# The Impact of Digital Technology

Digital technology has transformed modern life. In this module, we will examine the impact of digital technology, considering the history and ethical impact of the Internet, web, and the Internet of things (IoT) that we interact with on a daily basis.

## Introduction

Digital technology and digital electronics allow us to manipulate all types of information as digits (1s and 0s) in order to store and manage the information more efficiently and effectively. Digital technology, including all forms of computers, has extended human capabilities by allowing us to manipulate and explore information in ever-faster, more flexible, and more creative ways. The information can be expressed through words, numbers, sounds, and images. By better understanding digital technology, we improve our control over information, our lives, and our world.

### Consider This

Consider the impact of digital technologies on your daily life. If you are like many people, your day might start with your mobile phone’s alarm app waking you. The smartphone is one of many digital devices that have dramatically changed people’s lives, levels of connectivity, and productivity. Throughout the day, you probably use your phone to communicate with friends and perhaps business associates, look up information on the web, check your friends’ Facebook statuses, snap photos and post them online, send email, plan your evening, play games, watch movies, and listen to music. All of these activities are possible because information and media are digital and easy to manipulate with the tiny but powerful processor in your phone.

While digital devices like smartphones, laptops, tablets, and hi-def TVs impact our daily lives, they also make businesses more productive and successful. Digital information systems provide critical information to decision makers at just the right time to optimize a business strategy. Digital technologies also make it possible to crunch large quantities of scientific data to make amazing discoveries and life-saving products. From the human genome to the possibility of life in distant galaxies, digital technologies make it possible to collect and process huge amounts of information—information that might otherwise escape our notice.

As you read this unit, consider all the ways you interact with digital information and media each day, and the devices you use to access it. How does digital technology assist professionals in areas that interest you? What is the greatest discovery or most important research conducted that would not have been possible without digital technologies? Digital technologies are a major influence on everyone’s lives. Representing all types of information digitally allows that information to be stored, transported, shared, and manipulated in ways previously not possible. Digital technologies have become important tools in all professions and in managing our digital lives. Students in all majors spend a considerable amount of time learning the digital technologies that will assist them in their future professions.

Many of the valuable tools that we interact with everyday use digital technology to increase our productivity, increase our capabilities, entertain us, and expand our experiences. Some digital technologies have become essential to modern life.

Before delving into hardware, software, networks, business systems, and other high-level topics, it is important to grasp basic principles of digital technology. This section introduces the basics and establishes a vocabulary for discussing higher-level concepts.

### Ethical Dilemmas

Some fear that digital technologies are making life less personal and more stressful. Since the advent of digital technologies, software has made it possible for people to become many times more productive than they used to be. However, the pressure to be more productive can take its toll. Most of us lead a digital lifestyle, which involves looking at computer displays as much, if not more than, looking at the world around us. Such a lifestyle is not “natural,” argue some, while others counter this is just another step in human evolution. Some see technology and the digitization of the world as the end of life as we know it, while others view it as a vast and exciting new frontier. Some think technology is turning us all into robots, while others see it unlocking our true human potential.

Whichever point of view you might agree with, or if you have your own unique point of view, certain facts remain. Some people do have issues with various forms of computer addiction. Deadly accidents have increased dramatically due to distracted driving, including the use of mobile phones when drivers should be watching the road. Social etiquette is changing with each new use of technology as a tool for interacting with others. Many of us communicate less face-to-face than we do online. The digitization of media and information has caused serious concerns over intellectual property rights, privacy, and information security. In fact, dozens of ethical and societal issues have arisen or become exacerbated because of our use of digital technologies.

What do you think? Do the problems raised by digital technologies outweigh the benefits? Consider this as you read about the basics of digital technologies..

### Digital Literacy

Digital literacy refers to an understanding of how computers represent different types of data with digits and how the usefulness of that representation assists people in leading productive lives.

Digital literacy has become a requirement for most careers and a valuable asset for leading a productive, fulfilling life. Most colleges and many high schools now require students to prove they are competent with computers and digital technology in order to graduate. Digital literacy is a fundamental difference between the economies of successful, developed countries and those of struggling countries. Today, digital literacy is closely aligned with financial opportunity and independence.

This section on digital literacy—or the basic concepts underlying computers and digital technology—describes the most elementary building blocks of computers and other digital electronics. These topics provide a foundation for understanding all things digital.

For example, by understanding how movies are digitized and stored, you can understand why movies require much more storage capacity than music. This information could assist you in purchasing a computer or a cellular plan that best meets your movie consumption needs. Similarly, by understanding how computers represent all sorts of data as bits, you can get a sense of how today’s most popular mobile phones are able to provide so many functions. Understanding digital technologies empowered a college student named Mark Zuckerberg to establish the billion-dollar social networking business, Facebook. It also empowered citizens in Egypt and elsewhere to reclaim their governments. Digital technologies have changed lives in both subtle and dramatic ways.

Digital literacy begins with understanding what a computer is and seeing the value of computer literacy. Next, you learn how bits and bytes are used to represent words, values, sounds, music, photographs, drawings, movies, and other information that matters to people, in a digital format. You’ll also learn how a tiny device like a smartphone can provide all the hardware required for millions of apps.

This section also introduces the notion of technology ecosystems. Increasingly, companies like Apple, Google, Microsoft, and Amazon are manufacturing different types of devices—smartwatches, smartphones, tablets, and PCs—designed to work together. These technology ecosystems strongly impact the computers, devices, services, and software we purchase.

### Computers

A computer is a digital electronics device that combines hardware and software to accept the input of data, and then processes and stores the data to produce some useful output.

Computers have transformed the world. The ability to digitally represent and process information, including text, values, images, and sound, has allowed us to be more productive and to extend our intellect and understanding of life in this universe. The power of computing can improve every aspect of life, both professionally and personally. Along with a wide array of benefits, computers also generate a wide variety of ethical dilemmas, such as the digital divide, intellectual property rights, privacy, freedom of speech, and health-related issues associated with living a digital lifestyle.

Computers are made up of hardware—the tangible parts of the computer that take up physical space—and software—the electronic instructions that tell the computer what to do. The hardware and software work together to collect input, perform processing based on the input, store data, and produce output (useful results).

Input can be anything that a computer can collect, such as keyboard characters, mouse or touchpad movements, menu selections on a cell phone made with keyboard arrows or voice commands, music streamed from an Internet server to a smartphone, or a heartbeat monitored by medical sensors.

A computer performs processing using an integrated circuit designed to manipulate bits in a manner that carries out the instructions of the software. Data can be stored in a computer—that is, held temporarily or permanently, either by using electrical charges or magnetic particles on disks or tapes or by burning pits into the surface of a CD, DVD, or Blu-ray disc.

A computer produces output—the results of the processing. Output may take the form of images or words shown on a display printed to paper; sounds pumped through speakers or headphones; or alerts vibrating on a wrist or in a pocket. Output from a computer can also be used as input into another computing process. One of the primary purposes of a computer is to process data—bytes stored in a computer that may represent numbers, characters, sounds, or colors—into information; that is, data organized in such a way that provides value to the user, such as music, images, the result of calculations, or useful information from a database query.

Computers are classified as either general-purpose or special-purpose. A general-purpose computer is designed to carry out a wide variety of activities, depending on the software being used. General-purpose computers include personal computers (PCs), mobile computers, servers, and supercomputers. A special-purpose computer is designed for a specific computing purpose. Game devices, digital music players, digital cameras, GPS navigators, digital thermostats and other smart appliances, and a host of industry-specific devices, such as pacemakers, airport security scanners, and bank ATMs, are all special-purpose computers.

Computers are used in five primary areas to improve our lives, as detailed in the following table:

Computer Use Description
Computation Computers’ computational power—their ability to calculate and analyze—is solving many of life’s biggest mysteries, such as balancing your budget as well as unraveling the mysteries of protein folding to find cures for numerous diseases.
Automation Computers automate otherwise human actions and tasks, such as identifying explosive materials and safely disposing of them, as well as recording your favorite television program while you are away.
Entertainment Computers provide entertainment through digital music and media players, interactive games, motion picture special effects, and many other forms of entertainment.
Managing information Computers manage information, using databases that can filter and query billions of records to deliver the needed information.

### Technology Ecosystem

A technology ecosystem is a family of devices and software designed by the manufacturer to work together.

Technology touches every aspect of our lives. If you were to map out all of the hardware and software you depend on, you would end up with a diagram similar to the one below. Cloud computing has made it possible for our devices to work together, harmoniously sharing files and apps across devices.

This sharing works best when all of your devices are made by the same manufacturer. Consumers are increasingly pressured to buy into one manufacturer’s ecosystem.

Purchasing the latest gadget can have unforeseen consequences. Sure, the Apple Watch is cool, but if you purchase it, you will have to own an iPhone for it to connect to; an Android phone won’t do. If you own an iPhone, it makes sense to own an iPad so you can share apps between the two. You’ll want to use a Macbook as well so that all of your devices can sync using iCloud, Apple’s cloud service. Apple iCloud provides free access to Apple apps, including Calendar, Contacts, Reminders, Mail, Pages, Numbers, and Keynote. It makes sense to use these apps rather than Microsoft or Google apps, since they are provided on all of your Apple devices and are programmed to back up documents in iCloud. Your music and movies will have to go in iTunes, and your books and magazine subscriptions in iBooks. Oh, and by the way, you should really hang out with other people who use Apple products so you can utilize Facetime and Messages— Apple’s video chat and text messaging apps, which only connect you with other Apple users. While Apple may have one of the most restrictive technology ecosystems (sometimes called a walled garden), Microsoft, Google, and others are employing similar strategies to win consumers over to their family of products.

It is possible to utilize multiple ecosystems, but you’ll suffer some inconveniences. For example, Google has apps that run on iPhone to allow Apple users to use Google Docs, Google Now, and Gmail. Similarly, Apple iTunes is available for Android and Windows devices. Amazon is also working to build an ecosystem with its Kindle ereaders and tablets, Fire TV, Amazon Music, and Amazon Prime subscription service that provides free shipping, movies, music, and ebooks.

Smart consumers will analyze their technology use and review all of the hardware, software, and services provided by each company’s ecosystem to determine which is the best fit. Technology ecosystems can have a huge impact on our day-to-day lives with systems that extend far beyond traditional computing into our homes, cars, entertainment, health care, and finances.

### History of the Computer

Computers were, prior to the invention of electricity, and its subsequent application to programmed mathematical and logical calculations, any mechanical device or person that performed these same calculations.

Mathematics and formal logic are disciplines that have been studied and practiced by humans throughout recorded history. Unlike more subjective disciplines, such as art, music, and literature, math and logic follow strict, unambiguous rules that solve specific problems with regular, reproducible procedures, a fact that did not escape the notice of mechanical engineers and inventors.

Programming Pioneer - Ada Lovelace

Ada Lovelace in 1852. Authored by: Henry Phillips, License: CC0 1.0 Universal (CC0 1.0)
Before the development of electronic computers, numerous mechanical devices were designed and built to perform many of the same tasks now handled by digital devices. In some way, mechanical clocks can be considered special-purpose mechanical computers, and the Antikythera mechanism, found in a shipwreck off the coast of the Greek island of the same name, may have been an early, but surprisingly sophisticated, navigational computer.

In many ways, all modern computers are successors to Charles Babbage’s Analytical Engine, and all programmers are heirs to the pioneering code written for it by Ada Lovelace.

### Computing History

The history of the electronic computer dates from around 1940 where it played a significant role in World War II, initially using electric relays for processing, progressing to vacuum tubes, then to diodes and transistors, and finally to the integrated circuits used today.

Humankind has progressed through a series of technological revolutions, each of which has propelled us to a higher level of existence. The agricultural revolution brought a dependable source of food production; the scientific revolution brought higher-level thinking and understanding through mathematics, physics, astronomy, biology, and chemistry; the industrial revolution brought more productive manufacturing processes; and the digital revolution provided us with higher-order computational capabilities, advanced communications, and powerful information processing. The computer is responsible for ushering us into the digital age. Understanding its evolution assists us in understanding its impact on humanity.

Early computers such as ENIAC, filled large rooms with cabinets full of vacuum tubes, diodes, relays, resistors, capacitors, and wiring.

ENIAC Computer - The size of a room

U.S. Army. Authored by: Collections of the University of Pennsylvania Archives, License: Public Domain
During the late 1930s, a German engineer by the name of Konrad Zuse, began working on a machine that could carry out Boolean operations (those that govern processing with 1s and 0s) using electrical relays like those used in telephones. Zuse’s third generation of that machine, named the Z3, was completed in 1941 and is considered by most to be the first working programmable, fully automatic, digital computer. The Z3 was used to perform statistical analyses of wing flutter.

Zuse’s work was cut short by World War II because computer research was not considered “war-important.” Ironically, at the same time Zuse lost his funding, enemies of Germany were busy building computers to use against Germany.

Several generations of early computers were funded by the war effort. An electro-mechanical machine called Enigma, invented by the German engineer Arthur Scherbius at the end of World War I (1918), was used for enciphering and deciphering secret messages. In 1939, at the start of World War II, England created a more sophisticated device called the Bombe, to help decipher German Enigma-machine-encrypted secret messages. The battle of ciphers and code breaking became increasingly complex between Germany and its enemies. In 1943, Colossus—the first programmable electronic digital computer—was developed by British codebreakers. Colossus used 1,500 vacuum tubes and rolls of punched paper tape to test code solutions, reducing the time required to crack a code from weeks to hours.

Shortly after the war, in 1946, a computer named ENIAC was designed by John Mauchly and J. Presper Eckert to calculate artillery firing tables for the United States Army’s Ballistic Research Laboratory. ENIAC is considered the first fully electronic general-purpose computer. One of ENIAC’s first jobs was to study the feasibility of the hydrogen bomb. ENIAC contained 17,468 vacuum tubes, 7,200 crystal diodes, 1,500 relays, 70,000 resistors, 10,000 capacitors, and around 5 million hand-soldered joints. People joked that whenever ENIAC was switched on, lights in Philadelphia dimmed. ENIAC’s input and output utilized punched cards. ENIAC inspired a whole generation of computers that used vacuum tubes to store and route bits of data.

In 1947, the transistor was unveiled by Bell Labs. Transistors were able to function like vacuum tubes but were much smaller and more reliable. Computer engineers gradually moved from vacuum tubes to transistors for processing. In the late 1950s, Jack Kilby of Texas Instruments and Robert Noyce of Fairchild Semiconductor developed a method to integrate multiple transistors into a single module called an integrated circuit. The invention of the integrated circuit (also called microchips or simply chips) allowed electrical engineers to create more complicated electrical circuits in a much smaller package.

Today, chips utilize silicon-based transistors with two types of silicon, called n-type silicon and p-type silicon, for the negative and positive aspects of their electrons. By applying and removing voltage to the gate and drain of the transistor, electrons are allowed to flow through the transistor or are stopped. When electricity is flowing through a transistor, it represents a 1; when it is not flowing, it represents a 0. By combining transistors and using the output from one or more transistors as the input to others, computers control the flow of electricity in a manner that represents mathematical and logical operations.

The trend in digital technologies has always been toward smaller, more powerful, and faster devices. Chip manufacturers strive to reduce the size of transistors and the connections between them so that electrons flow more quickly through the circuits. A circuit is created by combining transistors, and sometimes other components, in a manner that accomplishes a specific task. Intel was able to create a circuit for storing data in flash memory that is only 25 nanometers in size. It would take about 4,000 of these microscopic circuits to span the width of a human hair. The transistors in today’s processors are so small that over 2 billion can be stored on a surface the size of your thumbnail.

### Moore’s Law

Moore’s Law provides an understanding of the exponential rate of advancement in processor technologies. A processor that has double the amount of transistors is twice as fast and powerful. Using Moore’s Law, we can gauge how fast processors might be in coming years.

Moore’s Law states the number of transistors on a chip will double about every two years. This is illustrated by the fact that in 2006, there were 1 billion transistors on Intel’s most powerful chip; and in 2008, Intel announced a chip with 2 billion transistors. Similarly, in 2008, CPUs were engineered using 45 nm technology; in 2009, it shrunk to 32 nm; in 2012, 22 nm; and now we’re at 14 nm processors built into 3D processing cubes for even faster processing. Utilizing Moore’s Law, one could guess at processor size and speeds over the next decade.

Moore's Law

MC13224. Authored by Travis Goodspeed, License: CC BY 2.0
It is also important to note that most experts, including Moore himself, believe that Moore’s Law will eventually exhaust itself as transistors become too small to be created out of silicon. Intel started using hafnium in place of silicon to create transistor gates and reduce the scale of transistors from 75 nm to 32 nm. Such a reduction would have been impossible using silicon gates due to the physical properties of silicon. Many other materials and processing technologies are being studied in order to continue, if not speed up, the progress of technological innovation. For example, IBM was able to store one bit of data using only 12 iron atoms, using an unconventional form of magnetism. Japanese scientists built a processor from a single layer of molecules that calculates in parallel like the human brain. Google’s D-Wave quantum computer is able to process and run certain operations 100 million times faster than a traditional silicon processor. The table below lists some of the most promising new processing technologies.

### New Processing Technologies

Technology Description
High-k materials The use of materials with a high dielectric constant (k), such as hafnium and zirconium, creates smaller transistors than silicon can support.
Optical computing Some companies are experimenting with chips called optical processors that use light waves instead of electrical current. The primary advantage of optical processors is their speed. It has been estimated that optical processors have the potential to be 500 times faster than traditional electronic circuits.
Three-dimensional processing Recently, Intel has made the transition from two dimensions on a thin wafer to three dimensions on a cube. Stacking wafers into 3D arrays speeds processing by allowing it to occur horizontally and vertically simultaneously.
Quantum computing Quantum computing proposes the manipulation of quantum states to perform computations far faster than is possible on any conventional computer. A quantum computer doesn’t use bits, but rather a fundamental unit of information called a quantum bit or qubit. The qubit displays properties in adherence to the laws of quantum mechanics, which differ radically from the laws of classical physics.
DNA computing DNA computing, or molecular computing, is a very promising new technology emerging from nanotechnology and based on DNA. Israeli scientists built the first “programmable molecular computing machine,” composed of enzymes and DNA molecules, that can perform 330 trillion operations per second—more than 100,000 times the speed of the fastest PC.

[Next](./2.md)

[Table of Contents](./README.md)
